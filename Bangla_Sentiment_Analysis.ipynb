{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BnSent_Bin_NafisAlviVai.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0xT3cDQaJr-"
      },
      "source": [
        "# Copyright: Abdul Hasib Uddin <abdulhasibuddin@gmail.com>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QadLL8vtZnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481aecbc-a609-40fc-c659-847a4d721662"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6ub9D1Ata7L"
      },
      "source": [
        "work_dir = \"drive/My Drive/Sentiment_Analysis/\"\n",
        "#impl_type = \"GRU\"\n",
        "#dataset_name = \"BanglaSentiment_Bin_NafisAlvi\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v-pwLMLta-I"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "#os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,40).__str__()\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler, LambdaCallback\n",
        "from IPython.display import display\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.callbacks import CSVLogger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjJXg0z_tbOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffdf9ba-b088-4724-b7a5-ac5e2be0d302"
      },
      "source": [
        "#from google.colab import files\n",
        "'''\n",
        "files.upload()\n",
        "files.upload()\n",
        "#'''\n",
        "\n",
        "with open(work_dir+'BanglaSentiment_Bin_Data.txt', 'r', encoding=\"utf8\") as f:\n",
        "    dataset = f.read()\n",
        "with open(work_dir+'BanglaSentiment_Bin_Label.txt', 'r', encoding=\"utf8\") as f:\n",
        "    labelset = f.read()\n",
        "\n",
        "print('Done file uploading!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done file uploading!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxr18sirPaFZ"
      },
      "source": [
        "#dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUaWr9QTtbUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb95ecac-c42e-4242-a20d-c866ff517fbd"
      },
      "source": [
        "list_data = dataset.split(\"\\n\")\n",
        "list_label = labelset.split(\"\\n\")\n",
        "\n",
        "print(\"len(list_data) =\", len(list_data))\n",
        "print(\"len(list_label) =\", len(list_label))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(list_data) = 7000\n",
            "len(list_label) = 7000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtJ4_TkQ4Xbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174dce33-7fca-4b5a-f350-ee1e189e7d03"
      },
      "source": [
        "list_data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' সাকিব মাশরাফির যোগ্য সহচর  মাশরাফি এই টিমের নিউক্লিয়াস হইলে সাকিব এই টিমের পাওয়ার হাউজ অর্থাৎ মাইট্রোকন্ড্রিয়া ',\n",
              " ' কথাটা সত্য বলেছেন এক ব্যক্তির ক্ষমতা তাই আওয়ামীলীগ বলে লাভ কি প্রধানমন্ত্রী কথা বলাই ভালসাধারণ সম্পাদক উনি বানাতে পারে উনি আবার ',\n",
              " ' ওরে মারার জন্য যা লাগে তাই দিব ',\n",
              " ' তামিম ভাই আপনাকে ধন্যবাদ একশ রান করার জন্য ',\n",
              " ' কুরআন শরিফে খুব সুন্দার করে এর বর্ননা দেওয়া হয়েছে ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upcOVB7iy0m6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02238dd-75c0-4671-9d75-a4faeb3f45c4"
      },
      "source": [
        "print(list_label[0])\n",
        "print(list_label[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n",
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri7axisNyRvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5004f93e-4b2a-41e6-a2ba-37673416fda9"
      },
      "source": [
        "list_label[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGYBUQX36BEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0f89d7-2c32-4887-ba58-4ec3c80a66fd"
      },
      "source": [
        "#'''\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(list_data)\n",
        "# summarize what was learned\n",
        "#print(t.word_counts)\n",
        "#print(t.document_count)\n",
        "#print(t.word_index)\n",
        "#print(t.word_docs)\n",
        "# integer encode documents\n",
        "encoded_dataset = t.texts_to_matrix(list_data, mode='count')\n",
        "#print(encoded_dataset)\n",
        "print(encoded_dataset[:5])\n",
        "print(encoded_dataset[0][10])\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxWrvJxEOzMy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "ef4f3766-e62a-4ebd-8c91-ce12812f0eaa"
      },
      "source": [
        "'''\n",
        "from keras.preprocessing.text import hashing_trick\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "#text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(dataset))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "#encoded_dataset = hashing_trick(dataset, round(vocab_size*1.3), hash_function='md5')\n",
        "\n",
        "encoded_dataset = []\n",
        "for data in list_data:\n",
        "    encoded_dataset.append(hashing_trick(data, round(vocab_size*1.3), hash_function='md5'))\n",
        "\n",
        "print(\"len(encoded_dataset) =\", len(encoded_dataset))\n",
        "print(\"len(encoded_dataset[0]) =\", len(encoded_dataset[0]))\n",
        "print(\"len(encoded_dataset[-1]) =\", len(encoded_dataset[-1]))\n",
        "print(encoded_dataset[:5])\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom keras.preprocessing.text import hashing_trick\\nfrom keras.preprocessing.text import text_to_word_sequence\\n# define the document\\n#text = \\'The quick brown fox jumped over the lazy dog.\\'\\n# estimate the size of the vocabulary\\nwords = set(text_to_word_sequence(dataset))\\nvocab_size = len(words)\\nprint(vocab_size)\\n# integer encode the document\\n#encoded_dataset = hashing_trick(dataset, round(vocab_size*1.3), hash_function=\\'md5\\')\\n\\nencoded_dataset = []\\nfor data in list_data:\\n    encoded_dataset.append(hashing_trick(data, round(vocab_size*1.3), hash_function=\\'md5\\'))\\n\\nprint(\"len(encoded_dataset) =\", len(encoded_dataset))\\nprint(\"len(encoded_dataset[0]) =\", len(encoded_dataset[0]))\\nprint(\"len(encoded_dataset[-1]) =\", len(encoded_dataset[-1]))\\nprint(encoded_dataset[:5])\\n#'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru1WzC6kQwN1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xlm495m6Q1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b6eab9-e5d4-4efc-b7cb-e95a8bcfc409"
      },
      "source": [
        "print(\"np.array(encoded_dataset).shape =\", np.array(encoded_dataset).shape)\n",
        "print(\"len(encoded_dataset[0]) =\", len(encoded_dataset[0]))\n",
        "print(\"len(encoded_dataset[-1]) =\", len(encoded_dataset[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "np.array(encoded_dataset).shape = (7000, 21889)\n",
            "len(encoded_dataset[0]) = 21889\n",
            "len(encoded_dataset[-1]) = 21889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVUgqLvsagU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "77ba562e-d736-47a7-fc69-865fe605cd74"
      },
      "source": [
        "'''\n",
        "zero_pad = np.zeros(encoded_dataset.shape)\n",
        "zero_pad[:a.shape[0],:a.shape[1]] = a\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nzero_pad = np.zeros(encoded_dataset.shape)\\nzero_pad[:a.shape[0],:a.shape[1]] = a\\n#'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiTTbubmXXw7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d369e240-371d-4818-96d3-fc159bbf5982"
      },
      "source": [
        "'''\n",
        "padded_dataset = np.pad(encoded_dataset, [(0, 1), (0, 1)], mode='constant', constant_values=0)\n",
        "print(\"len(padded_dataset[0]) =\", len(padded_dataset[0]))\n",
        "print(\"len(padded_dataset[-1]) =\", len(padded_dataset[-1]))\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\npadded_dataset = np.pad(encoded_dataset, [(0, 1), (0, 1)], mode=\\'constant\\', constant_values=0)\\nprint(\"len(padded_dataset[0]) =\", len(padded_dataset[0]))\\nprint(\"len(padded_dataset[-1]) =\", len(padded_dataset[-1]))\\n#'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZbEpJPfylrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db16e87-cbd5-4212-da18-aefd4bb1be2e"
      },
      "source": [
        "dataset_positive = []\n",
        "labelset_positive = []\n",
        "dataset_negative = []\n",
        "labelset_negative = []\n",
        "\n",
        "for data,label in zip(encoded_dataset,list_label):\n",
        "    if label == 'positive':\n",
        "        dataset_positive.append(data)\n",
        "        labelset_positive.append(1)\n",
        "    elif label == 'negative':\n",
        "        dataset_negative.append(data)\n",
        "        labelset_negative.append(0)\n",
        "\n",
        "print(\"len(dataset_positive) =\", len(dataset_positive))\n",
        "print(\"len(labelset_positive) =\", len(labelset_positive))\n",
        "print(\"len(dataset_negative) =\", len(dataset_negative))\n",
        "print(\"len(labelset_negative) =\", len(labelset_negative))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(dataset_positive) = 3500\n",
            "len(labelset_positive) = 3500\n",
            "len(dataset_negative) = 3500\n",
            "len(labelset_negative) = 3500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYWY73K81dfp"
      },
      "source": [
        "#print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8j7YvJ3tbXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79633e2f-0d11-4e36-df78-796c9a0a3f23"
      },
      "source": [
        "dataset_stratified = []\n",
        "labelset_stratified = []\n",
        "\n",
        "for i in range(len(labelset_positive)):\n",
        "    dataset_stratified.append(dataset_positive[i])\n",
        "    dataset_stratified.append(dataset_negative[i])\n",
        "\n",
        "    labelset_stratified.append(labelset_positive[i])\n",
        "    labelset_stratified.append(labelset_negative[i])\n",
        "\n",
        "print(\"len(dataset_stratified) =\", len(dataset_stratified))\n",
        "print(\"len(labelset_stratified) =\", len(labelset_stratified))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(dataset_stratified) = 7000\n",
            "len(labelset_stratified) = 7000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2263OewStbaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7edfb60-7d1d-4d95-a45e-4b700afa34bc"
      },
      "source": [
        "print(labelset_stratified[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AEOg55Ptbcw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "0d3af8e5-b3e8-4cb5-917b-c8e0bc6441a6"
      },
      "source": [
        "'''\n",
        "TRAIN_TEST_SPLIT = int(len(labelset_stratified)*0.5)\n",
        "print(\"TRAIN_TEST_SPLIT =\", TRAIN_TEST_SPLIT)\n",
        "\n",
        "train_val_dataset = dataset_stratified[TRAIN_TEST_SPLIT:]\n",
        "test_dataset = dataset_stratified[:TRAIN_TEST_SPLIT]\n",
        "\n",
        "train_val_labelset = labelset_stratified[TRAIN_TEST_SPLIT:]\n",
        "test_labelset = labelset_stratified[:TRAIN_TEST_SPLIT]\n",
        "\n",
        "print(\"len(train_val_dataset) =\", len(train_val_dataset))\n",
        "print(\"len(test_dataset) =\", len(test_dataset))\n",
        "print(\"len(train_val_labelset) =\", len(train_val_labelset))\n",
        "print(\"len(test_labelset) =\", len(test_labelset))\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nTRAIN_TEST_SPLIT = int(len(labelset_stratified)*0.5)\\nprint(\"TRAIN_TEST_SPLIT =\", TRAIN_TEST_SPLIT)\\n\\ntrain_val_dataset = dataset_stratified[TRAIN_TEST_SPLIT:]\\ntest_dataset = dataset_stratified[:TRAIN_TEST_SPLIT]\\n\\ntrain_val_labelset = labelset_stratified[TRAIN_TEST_SPLIT:]\\ntest_labelset = labelset_stratified[:TRAIN_TEST_SPLIT]\\n\\nprint(\"len(train_val_dataset) =\", len(train_val_dataset))\\nprint(\"len(test_dataset) =\", len(test_dataset))\\nprint(\"len(train_val_labelset) =\", len(train_val_labelset))\\nprint(\"len(test_labelset) =\", len(test_labelset))\\n#'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_X_IlWCRL_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0da992-b1d9-4b69-bd37-bb074984fbf4"
      },
      "source": [
        "num_folds = 10\n",
        "split_indices = int(len(labelset_stratified)/num_folds)\n",
        "fold = 3\n",
        "\n",
        "train_val_dataset = dataset_stratified[:fold*split_indices-split_indices]\n",
        "train_val_dataset += dataset_stratified[fold*split_indices:]\n",
        "test_dataset = dataset_stratified[fold*split_indices-split_indices:fold*split_indices]\n",
        "\n",
        "train_val_labelset = labelset_stratified[:fold*split_indices-split_indices]\n",
        "train_val_labelset += labelset_stratified[fold*split_indices:]\n",
        "test_labelset = labelset_stratified[fold*split_indices-split_indices:fold*split_indices]\n",
        "\n",
        "print(\"len(train_val_dataset) =\", len(train_val_dataset))\n",
        "print(\"len(test_dataset) =\", len(test_dataset))\n",
        "print(\"len(train_val_labelset) =\", len(train_val_labelset))\n",
        "print(\"len(test_labelset) =\", len(test_labelset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(train_val_dataset) = 6300\n",
            "len(test_dataset) = 700\n",
            "len(train_val_labelset) = 6300\n",
            "len(test_labelset) = 700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qipeXAfUBQ0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba50ee4c-e09f-486c-9851-f284898ae95b"
      },
      "source": [
        "np_train_val_dataset = np.array(train_val_dataset)\n",
        "np_test_dataset = np.array(test_dataset)\n",
        "np_train_val_labelset = np.array(train_val_labelset)\n",
        "np_test_labelset = np.array(test_labelset)\n",
        "\n",
        "print(\"np_train_val_dataset.shape =\", np_train_val_dataset.shape)\n",
        "print(\"np_test_dataset.shape =\", np_test_dataset.shape)\n",
        "print(\"np_train_val_labelset.shape =\", np_train_val_labelset.shape)\n",
        "print(\"np_test_labelset.shape =\", np_test_labelset.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "np_train_val_dataset.shape = (6300, 21889)\n",
            "np_test_dataset.shape = (700, 21889)\n",
            "np_train_val_labelset.shape = (6300,)\n",
            "np_test_labelset.shape = (700,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhO-xq25tbfv"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "shuffled_train_val_dataset, shuffled_train_val_labelset = shuffle(np_train_val_dataset, np_train_val_labelset) # , random_state=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3HfjVSaZHqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761e120d-56fd-4559-bd19-b35823fca8e5"
      },
      "source": [
        "print(shuffled_train_val_labelset[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 1 0 1 1 0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH8b6qL7DALK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-IK2TCNDAQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd7a2c0-abf3-45d7-d2d8-d7b37d4043f1"
      },
      "source": [
        "final_train_dataset = np.expand_dims(shuffled_train_val_dataset, axis = 2)\n",
        "final_test_dataset = np.expand_dims(np_test_dataset, axis = 2)\n",
        "\n",
        "y_train2 = np.expand_dims(shuffled_train_val_labelset, axis = 1)\n",
        "y_test2 = np.expand_dims(np_test_labelset, axis = 1)\n",
        "\n",
        "print('final_train_dataset.shape =', final_train_dataset.shape)\n",
        "print('final_test_dataset.shape =', final_test_dataset.shape)\n",
        "print('y_train2.shape =', y_train2.shape)\n",
        "print('y_test2.shape =', y_test2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final_train_dataset.shape = (6300, 21889, 1)\n",
            "final_test_dataset.shape = (700, 21889, 1)\n",
            "y_train2.shape = (6300, 1)\n",
            "y_test2.shape = (700, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yTnufDsDAUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6153d1a8-9ee9-4d44-f42f-8061840bfb3e"
      },
      "source": [
        "num_classes = 2\n",
        "final_train_label = tf.keras.utils.to_categorical(y_train2, num_classes)\n",
        "final_test_label = tf.keras.utils.to_categorical(y_test2, num_classes)\n",
        "\n",
        "print('final_train_label.shape =',final_train_label.shape)\n",
        "print('final_test_label.shape =',final_test_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final_train_label.shape = (6300, 2)\n",
            "final_test_label.shape = (700, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hB6FscJeiBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102e9303-17e6-4878-f064-2f955a53334b"
      },
      "source": [
        "final_train_label[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPkXSr06DAOh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9jmmAnrta0R"
      },
      "source": [
        "#'''\n",
        "epoch_initial = True\n",
        "count_no_improvement = 0\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJZfgv9hImGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc006a53-4382-4df6-ca8c-d2800f95d143"
      },
      "source": [
        "impl_type = \"GRU\" # LSTM GRU BiLSTM BiGRU Dense\n",
        "dataset_name = \"BnSent_Bin_Nafis\"\n",
        "\n",
        "NUM_NEURONS = 48 # 32+16\n",
        "#NUM_DENSE_NEURONS = 0 # x_train.shape[1]*x_train.shape[2]\n",
        "NUM_LAYERS = 5\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 3000\n",
        "#epochs_completed = 0\n",
        "LEARNING_RATE = 0.0001\n",
        "EPSILON = 1e-4\n",
        "DROPOUT = 0.5\n",
        "#REC_DROP = 0.5  \n",
        "\n",
        "'''\n",
        "ker_init = 'glorot_uniform' # 'glorot_uniform' 'zeros' 'ones'\n",
        "rec_init = 'orthogonal' # 'orthogonal' 'zeros' 'ones'\n",
        "b_init = 'zeros' # 'zeros' 'ones'\n",
        "#'''\n",
        "\n",
        "OPTIMIZER = \"Adam\" # \"RMSProp\" \"SGD\" \"Adam\" \"Adamax\" \"Adadelta\" \"Adagrad\" \"SGD\"\n",
        "LOSS = 'categorical_crossentropy' # binary_crossentropy categorical_crossentropy\n",
        "ACTIVATION_FUNCTION = 'default' # 'relu' 'LeakyReLU' 'PReLU' 'ELU' 'default'\n",
        "DENSE_ACTIVATION_FUNCTION = 'default' # 'relu' 'LeakyReLU' 'PReLU' 'ELU' 'default'\n",
        "FINAL_ACTIVATION_FUNCTION = 'softmax'\n",
        "validation_split = 0.1\n",
        "#kernel_size=(1,1)\n",
        "early_stop_after_epochs = 5\n",
        "chk_name_ext = \"_withGRU_L1_last_run3_2\" # \"_2\" _3 \"_rerun\" _withDenseL1_2 _withGRU_L1 _withLSTM_L1 noFlatten _withGRU_L1_last_2 _withGRU_L1_last_split0\n",
        "\n",
        "# Checkpointer with Dense:\n",
        "'''\n",
        "checkpointer_name  = \"weights.\"+dataset_name+\".\"+impl_type+\".\"+ACTIVATION_FUNCTION+\".dAct.\"+DENSE_ACTIVATION_FUNCTION+\".nLayers\"+\\\n",
        "                    str(NUM_LAYERS)+\".nNeurons\"+str(NUM_NEURONS)+\".dNeurons\"+str(NUM_DENSE_NEURONS)+\".opt\"+OPTIMIZER+\\\n",
        "                    \".batch\"+str(BATCH_SIZE)+\".dropout\"+str(DROPOUT)+chk_name_ext+\".hdf5\"\n",
        "#'''\n",
        "\n",
        "# Checkpointer without Dense:\n",
        "#''' # +\".recDrop.\"+str(REC_DROP)\n",
        "checkpointer_name  = \"weights.\"+dataset_name+\".\"+impl_type+\".nFolds\"+str(num_folds)+\".Fold\"+str(fold)+\\\n",
        "                    \".act.\"+ACTIVATION_FUNCTION+\".nLayers\"+str(NUM_LAYERS)+\".nNeurons\"+str(NUM_NEURONS)+\".opt\"+OPTIMIZER+\\\n",
        "                    \".batch\"+str(BATCH_SIZE)+\".dropout\"+str(DROPOUT)+\\\n",
        "                    \".loss.\"+LOSS+\".lr.\"+str(LEARNING_RATE)+chk_name_ext+\".hdf5\"\n",
        "#'''\n",
        "\n",
        "log_name = \"log_\"+checkpointer_name[8:-5]+\".log\"\n",
        "\n",
        "print('checkpointer_name =', checkpointer_name)\n",
        "print('log_name =', log_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpointer_name = weights.BnSent_Bin_Nafis.GRU.nFolds10.Fold3.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run3_2.hdf5\n",
            "log_name = log_BnSent_Bin_Nafis.GRU.nFolds10.Fold3.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run3_2.log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQZwIS_ZC33T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYn1Gq5VHoJd"
      },
      "source": [
        "def activation_function():\n",
        "    if ACTIVATION_FUNCTION == 'relu':\n",
        "        tf.keras.layers.ReLU(max_value=None, negative_slope=0, threshold=0)\n",
        "    elif ACTIVATION_FUNCTION == 'LeakyReLU':\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.3)\n",
        "    elif ACTIVATION_FUNCTION == 'PReLU':\n",
        "        tf.keras.layers.PReLU(tf.initializers.constant(0.3)) # \"zeros\"\n",
        "    elif ACTIVATION_FUNCTION == 'ELU':\n",
        "        tf.keras.layers.ELU(alpha=1.0)\n",
        "    elif ACTIVATION_FUNCTION == 'default':\n",
        "        return 'tanh'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXPqKLTYIw5J"
      },
      "source": [
        "def dense_activation():\n",
        "    if DENSE_ACTIVATION_FUNCTION == 'relu':\n",
        "        tf.keras.layers.ReLU(max_value=None, negative_slope=0, threshold=0)\n",
        "    elif DENSE_ACTIVATION_FUNCTION == 'LeakyReLU':\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.3)\n",
        "    elif DENSE_ACTIVATION_FUNCTION == 'PReLU':\n",
        "        tf.keras.layers.PReLU(tf.initializers.constant(0.3)) # \"zeros\"\n",
        "    elif DENSE_ACTIVATION_FUNCTION == 'ELU':\n",
        "        tf.keras.layers.ELU(alpha=1.0)\n",
        "    elif DENSE_ACTIVATION_FUNCTION == 'default':\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_394UsfHpBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e6c3a73b-3fc1-422f-b755-31cc38c58d54"
      },
      "source": [
        "'''\n",
        "def gru(x):\n",
        "    return GRU(\n",
        "        NUM_NEURONS, \n",
        "        return_sequences = True, \n",
        "        kernel_initializer = ker_init, # ker_init tf.keras.initializers.HeNormal()\n",
        "        recurrent_initializer = ker_init, # rec_init tf.keras.initializers.HeNormal()\n",
        "        bias_initializer = b_init,\n",
        "        dropout = DROPOUT, \n",
        "        #recurrent_dropout = REC_DROP,\n",
        "        activation = activation_function()\n",
        "        )(x)\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef gru(x):\\n    return GRU(\\n        NUM_NEURONS, \\n        return_sequences = True, \\n        kernel_initializer = ker_init, # ker_init tf.keras.initializers.HeNormal()\\n        recurrent_initializer = ker_init, # rec_init tf.keras.initializers.HeNormal()\\n        bias_initializer = b_init,\\n        dropout = DROPOUT, \\n        #recurrent_dropout = REC_DROP,\\n        activation = activation_function()\\n        )(x)\\n#'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NTkbsz1HwEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11b1ab5-3c47-443d-d155-fc19d6db791a"
      },
      "source": [
        "#DROPOUT = 0.8\n",
        "\n",
        "input_shape = final_train_dataset.shape[1:]\n",
        "\n",
        "# Input tensor shape\n",
        "inputs = Input(input_shape)\n",
        "x = inputs\n",
        "\n",
        "'''\n",
        "x = Dense(NUM_DENSE_NEURONS, activation=dense_activation())(inputs)\n",
        "#x = Dropout(DROPOUT)(x)\n",
        "\n",
        "for _ in range(NUM_LAYERS):\n",
        "    x = gru(x)\n",
        "#'''\n",
        "\n",
        "#'''\n",
        "\"\"\"\n",
        "if impl_type == \"Dense\":\n",
        "    x = GRU(NUM_NEURONS, return_sequences=True, activation=activation_function())(x)\n",
        "    #x = LSTM(NUM_NEURONS, return_sequences=True, activation=activation_function())(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "    x = Flatten()(x)\n",
        "#\"\"\"\n",
        "for _ in range(NUM_LAYERS):\n",
        "    if impl_type == \"GRU\":\n",
        "        x = GRU(NUM_NEURONS, return_sequences=True, dropout=DROPOUT, activation=activation_function())(x)\n",
        "    elif impl_type == \"BiGRU\":\n",
        "        x = Bidirectional(GRU(NUM_NEURONS, return_sequences=True, dropout=DROPOUT, activation=activation_function()))(x)\n",
        "    elif impl_type == \"LSTM\":\n",
        "        x = LSTM(NUM_NEURONS, return_sequences=True, dropout=DROPOUT, activation=activation_function())(x)\n",
        "    elif impl_type == \"BiLSTM\":\n",
        "        x = Bidirectional(LSTM(NUM_NEURONS, return_sequences=True, dropout=DROPOUT, activation=activation_function()))(x)\n",
        "    elif impl_type == \"Dense\":\n",
        "        #x = Flatten()(x)\n",
        "        x = Dense(NUM_NEURONS, activation=dense_activation())(x)\n",
        "        x = Dropout(DROPOUT)(x)\n",
        "#'''\n",
        "#\"\"\"\n",
        "if impl_type == \"Dense\":\n",
        "    x = GRU(NUM_NEURONS, return_sequences=True, activation=activation_function())(x)\n",
        "    #x = LSTM(NUM_NEURONS, return_sequences=True, activation=activation_function())(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "    x = Flatten()(x)\n",
        "#\"\"\"\n",
        "if impl_type != \"Dense\":\n",
        "    x = Flatten()(x)\n",
        "\n",
        "'''\n",
        "x = Dense(1024, activation=dense_activation())(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation=dense_activation())(x)\n",
        "x = Dropout(0.5)(x)\n",
        "#'''\n",
        "outputs = Dense(num_classes, activation=FINAL_ACTIVATION_FUNCTION)(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 21889, 1)]        0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 21889, 48)         7344      \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 21889, 48)         14112     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 21889, 48)         14112     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 21889, 48)         14112     \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 21889, 48)         14112     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1050672)           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 2101346   \n",
            "=================================================================\n",
            "Total params: 2,165,138\n",
            "Trainable params: 2,165,138\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXZ0bUDmH1yy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed38261a-747a-4163-aadf-db83b970bba1"
      },
      "source": [
        "# \"RMSProp\" \"SGD\" \"Adam\" \"Adamax\" \"Adadelta\" \"Adagrad\" \"SGD\"\n",
        "#optimizer = tf.keras.optimizers.RMSprop(lr = LEARNING_RATE, epsilon=EPSILON)\n",
        "\n",
        "if OPTIMIZER == \"RMSProp\":\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr = LEARNING_RATE, epsilon=EPSILON)\n",
        "elif OPTIMIZER == \"Adam\":\n",
        "    optimizer = tf.keras.optimizers.Adam(lr = LEARNING_RATE, epsilon=EPSILON, beta_1=0.9, beta_2=0.999)\n",
        "elif OPTIMIZER == \"Adamax\":\n",
        "    optimizer = tf.keras.optimizers.Adamax(lr = LEARNING_RATE, epsilon=EPSILON, beta_1=0.9, beta_2=0.999)\n",
        "elif OPTIMIZER == \"Adadelta\":\n",
        "    optimizer = tf.keras.optimizers.Adadelta(lr = LEARNING_RATE, epsilon=EPSILON, rho=0.95)\n",
        "elif OPTIMIZER == \"Adagrad\":\n",
        "    optimizer = tf.keras.optimizers.Adagrad(lr = LEARNING_RATE, epsilon=EPSILON, initial_accumulator_value=0.1)\n",
        "elif OPTIMIZER == \"SGD\":\n",
        "    optimizer = tf.keras.optimizers.SGD(lr = LEARNING_RATE, momentum=0.85)\n",
        "\n",
        "'''\n",
        "list_metrics = ['accuracy', #tf.keras.metrics.Accuracy(), \n",
        "                tf.keras.metrics.Precision(), \n",
        "                tf.keras.metrics.Recall(),\n",
        "                tf.keras.metrics.TruePositives(),\n",
        "                tf.keras.metrics.TrueNegatives(),\n",
        "                tf.keras.metrics.FalsePositives(),\n",
        "                tf.keras.metrics.FalseNegatives()]\n",
        "#'''\n",
        "\n",
        "model.compile(\n",
        "    #optimizer=OPTIMIZER,\n",
        "    optimizer=optimizer,\n",
        "    loss=LOSS,\n",
        "    metrics='accuracy' #['accuracy', 'Precision', 'Recall'], list_metrics\n",
        ")\n",
        "\n",
        "print(\"OPTIMIZER =\", OPTIMIZER)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OPTIMIZER = Adam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkgZYYPZCIDd"
      },
      "source": [
        "# save the best model with least validation loss\n",
        "checkpointer = ModelCheckpoint(filepath = work_dir+checkpointer_name, \n",
        "                               #monitor='val_accuracy',\n",
        "                               monitor='val_loss',\n",
        "                               save_weights_only=False,  \n",
        "                               mode='auto', \n",
        "                               verbose = 0, # 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        "                               save_best_only =False\n",
        "                               )\n",
        "checkpointer_best = ModelCheckpoint(filepath = work_dir+\"best_\"+checkpointer_name, \n",
        "                                    monitor='val_loss', \n",
        "                                    save_weights_only=False,\n",
        "                                    mode='auto',  \n",
        "                                    verbose = 1, \n",
        "                                    save_best_only = True\n",
        "                                    )\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=early_stop_after_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oCD4L_0CIG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07be500f-34a8-46ad-b5e4-f8399a7aefb0"
      },
      "source": [
        "'''\n",
        "if 'count_no_improvement' not in globals():\n",
        "    count_no_improvement = 0\n",
        "    print(\"count_no_improvement =\", count_no_improvement)\n",
        "#'''\n",
        "'''\n",
        "count_no_improvement = 0\n",
        "epoch_initial = False\n",
        "#'''\n",
        "\n",
        "min_delta = 0.0001\n",
        "print(\"count_no_improvement =\", count_no_improvement)\n",
        "\n",
        "def checkBestPerformance(epoch, logs):\n",
        "    save_filepath = work_dir+\"best_\"+checkpointer_name\n",
        "    \n",
        "    global epoch_initial\n",
        "    if epoch_initial == True:\n",
        "        epoch_initial = False\n",
        "        model.save(filepath = save_filepath)\n",
        "        print(\". Model saved!\")\n",
        "\n",
        "    elif epoch_initial == False:\n",
        "        global count_no_improvement\n",
        "\n",
        "        log_data = pd.read_csv(work_dir+log_name, sep=',', usecols=['val_loss', 'val_accuracy'], engine='python')\n",
        "        min_val_loss = float(str(min(log_data.val_loss.values))[:6])\n",
        "        max_val_acc = float(str(max(log_data.val_accuracy.values))[:6])\n",
        "\n",
        "        current_val_acc = float(str(logs['val_accuracy'])[:6])\n",
        "        current_val_loss = float(str(logs['val_loss'])[:6])\n",
        "\n",
        "        if (current_val_loss < min_val_loss) and (abs(current_val_loss-min_val_loss) >= min_delta):\n",
        "            count_no_improvement = 0\n",
        "            model.save(filepath = save_filepath)\n",
        "            print(\"\\nval_loss decreased from\",min_val_loss,\" to\",current_val_loss,\"( val_accuracy =\",current_val_acc,\").\")\n",
        "\n",
        "        elif (current_val_loss==min_val_loss) and (current_val_acc>max_val_acc):\n",
        "            count_no_improvement = 0\n",
        "            model.save(filepath = save_filepath)\n",
        "            print(\"\\nval_accuracy increased to\", current_val_acc, \".\")\n",
        "\n",
        "        else:\n",
        "            count_no_improvement += 1\n",
        "            print(\". count_no_improvement =\", count_no_improvement)\n",
        "\n",
        "        if count_no_improvement >= early_stop_after_epochs:\n",
        "            global list_callbacks\n",
        "            del list_callbacks, count_no_improvement\n",
        "            #print(\"count_no_improvement =\", count_no_improvement, \"... list_callbacks =\", list_callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count_no_improvement = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWRXjfnECIBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f460b314-f1da-4296-8348-48a572a4d9b1"
      },
      "source": [
        "epochs_completed = 0\n",
        "list_callbacks = []\n",
        "csv_logger = CSVLogger(work_dir+log_name, separator=',', append=True)\n",
        "\n",
        "#if 'list_callbacks' in globals():\n",
        "#    del list_callbacks\n",
        "\n",
        "try:\n",
        "    log_data = pd.read_csv(work_dir+log_name, sep=',', usecols=['epoch'], engine='python')\n",
        "    epochs_completed = log_data.shape[0]\n",
        "\n",
        "    #if epochs_completed > 0:\n",
        "    model = load_model(work_dir+checkpointer_name)\n",
        "    list_callbacks = [checkpointer, LambdaCallback(on_epoch_end=checkBestPerformance), csv_logger]\n",
        "    print(\"epochs_completed =\", epochs_completed)\n",
        "    \n",
        "except Exception as error:\n",
        "    if epochs_completed == 0:\n",
        "        # list_callbacks = [checkpointer, checkpointer_best, csv_logger, early_stopping] \n",
        "        list_callbacks = [checkpointer, LambdaCallback(on_epoch_end=checkBestPerformance), csv_logger]\n",
        "        print(\"epochs_completed =\", epochs_completed)\n",
        "    elif epochs_completed > 0:\n",
        "        print(error)\n",
        "\n",
        "print('checkpointer_name =', checkpointer_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs_completed = 0\n",
            "checkpointer_name = weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold3.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run3.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtxANQeaCH8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b717a010-19fe-48cb-ae66-ff20da57f5ba"
      },
      "source": [
        "print('checkpointer_name =', checkpointer_name)\n",
        "print(\"Previously completed epochs =\", epochs_completed)\n",
        "print(\"count_no_improvement =\", count_no_improvement, \"\\n\")\n",
        "\n",
        "#'''\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    history = model.fit(final_train_dataset, final_train_label, \n",
        "                        shuffle=True, \n",
        "                        batch_size = BATCH_SIZE, \n",
        "                        epochs = NUM_EPOCHS - epochs_completed, \n",
        "                        #steps_per_epoch = 2,\n",
        "                        #validation_split = validation_split, \n",
        "                        validation_data = (final_test_dataset, final_test_label),\n",
        "                        callbacks=list_callbacks\n",
        "                        )\n",
        "    elapsed_time = time.time() - start_time \n",
        "    print(\"\\nTime elapsed: \", elapsed_time)\n",
        "\n",
        "except Exception as error:\n",
        "    print(\"\\nError:\", error)\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpointer_name = weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold3.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run3.hdf5\n",
            "Previously completed epochs = 0\n",
            "count_no_improvement = 0 \n",
            "\n",
            "Epoch 1/3000\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.5335. Model saved!\n",
            "99/99 [==============================] - 82s 825ms/step - loss: 0.6913 - accuracy: 0.5335 - val_loss: 0.6832 - val_accuracy: 0.5329\n",
            "Epoch 2/3000\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.6727 - accuracy: 0.6668\n",
            "val_loss decreased from 0.6832  to 0.6281 ( val_accuracy = 0.7657 ).\n",
            "99/99 [==============================] - 82s 832ms/step - loss: 0.6727 - accuracy: 0.6668 - val_loss: 0.6281 - val_accuracy: 0.7657\n",
            "Epoch 3/3000\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.7889\n",
            "val_loss decreased from 0.6281  to 0.4754 ( val_accuracy = 0.8299 ).\n",
            "99/99 [==============================] - 82s 825ms/step - loss: 0.5783 - accuracy: 0.7889 - val_loss: 0.4755 - val_accuracy: 0.8300\n",
            "Epoch 4/3000\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.3782 - accuracy: 0.8748\n",
            "val_loss decreased from 0.4754  to 0.3478 ( val_accuracy = 0.8614 ).\n",
            "99/99 [==============================] - 81s 821ms/step - loss: 0.3782 - accuracy: 0.8748 - val_loss: 0.3479 - val_accuracy: 0.8614\n",
            "Epoch 5/3000\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.1848 - accuracy: 0.9314\n",
            "val_loss decreased from 0.3478  to 0.2942 ( val_accuracy = 0.8885 ).\n",
            "99/99 [==============================] - 81s 819ms/step - loss: 0.1848 - accuracy: 0.9314 - val_loss: 0.2942 - val_accuracy: 0.8886\n",
            "Epoch 6/3000\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9540\n",
            "val_loss decreased from 0.2942  to 0.2907 ( val_accuracy = 0.8828 ).\n",
            "99/99 [==============================] - 81s 818ms/step - loss: 0.1164 - accuracy: 0.9540 - val_loss: 0.2908 - val_accuracy: 0.8829\n",
            "Epoch 7/3000\n",
            "59/99 [================>.............] - ETA: 30s - loss: 0.0881 - accuracy: 0.9648"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KHUcvmgtay_"
      },
      "source": [
        "csv_logger = CSVLogger(work_dir+log_name, separator=',', append=True)\n",
        "log_data = pd.read_csv(work_dir+log_name, sep=',', usecols=['epoch'], engine='python')\n",
        "epochs_completed = log_data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL4VENIeGe50"
      },
      "source": [
        "model_loaded = load_model(work_dir+\"best_\"+checkpointer_name)\n",
        "print(\"Loaded \"+work_dir+\"best_\"+checkpointer_name+\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AqGw8rpGe_s"
      },
      "source": [
        "'''\n",
        "Test Acc: 0.7143, Test Loss: 0.5672: ep12, best_weights.BanglaSentiment_Bin_NafisAlvi.GRU.act.default.nLayers3.nNeurons16.optAdam.batch64.dropout0.5.loss.categorical_crossentropy_withDenseL1_2.hdf5\n",
        "Test Acc: 0.6986, Test Loss: 0.5704: ep9,  best_weights.BanglaSentiment_Bin_NafisAlvi.LSTM.act.default.nLayers3.nNeurons16.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.hdf5\n",
        "Test Acc: 0.6800, Test Loss: 0.6241: ep9,  best_weights.BanglaSentiment_Bin_NafisAlvi.BiGRU.act.default.nLayers3.nNeurons16.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.hdf5\n",
        "-\n",
        "Test Acc: 0.6643, Test Loss: 0.6049: ep18, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers3.nNeurons16.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.hdf5\n",
        "Test Acc: 0.6771, Test Loss: 0.6042: ep12, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers3.nNeurons32.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.hdf5\n",
        "Test Acc: 0.6900, Test Loss: 0.5965: ep16, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optRMSProp.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001.hdf5\n",
        "Test Acc: 0.6829, Test Loss: 0.5981: ep17, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optRMSProp.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_2.hdf5\n",
        "Test Acc: 0.6843, Test Loss: 0.6070: ep147, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optSGD.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_2.hdf5\n",
        "Test Acc: 0.6771, Test Loss: 0.6079: ep101, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optSGD.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_3.hdf5\n",
        "Test Acc: 0.6800, Test Loss: 0.6082: ep138, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons64.optSGD.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001.hdf5\n",
        "Test Acc: 0.6743, Test Loss: 0.6044: ep220, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers3.nNeurons64.optSGD.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001.hdf5\n",
        "Test Acc: 0.6714, Test Loss: 0.6100: ep18, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons64.optRMSProp.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001.hdf5\n",
        "Test Acc: 0.6771, Test Loss: 0.6083: ep16, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons128.optRMSProp.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001.hdf5\n",
        "Test Acc: 0.6757, Test Loss: 0.6049: ep139, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons128.optSGD.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001.hdf5\n",
        "Test Acc: 0.6757, Test Loss: 0.6237: ep14, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optRMSProp.batch8.dropout0.5.loss.categorical_crossentropy.lr.0.0001.hdf5\n",
        "Test Acc: 0.6829, Test Loss: 0.5969: ep9, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001noFlatten.hdf5\n",
        "\n",
        "-\n",
        "Test Acc: 0.7371, Test Loss: 0.5529: ep13, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optRMSProp.batch8.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1.hdf5\n",
        "Test Acc: 0.7314, Test Loss: 0.5575: ep11, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optRMSProp.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1.hdf5\n",
        "Test Acc: 0.7129, Test Loss: 0.5963: ep11, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optRMSProp.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L3.hdf5\n",
        "Test Acc: 0.7429, Test Loss: 0.5550: ep9,  best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1.hdf5\n",
        "Test Acc: 0.7071, Test Loss: 0.5634: ep8,  best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withLSTM_L1.hdf5\n",
        "Test Acc: 0.7614, Test Loss: 0.5159: ep11, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last.hdf5\n",
        "Test Acc: 0.7557, Test Loss: 0.4976: ep11, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_2.hdf5\n",
        "Test Acc: 0.7657, Test Loss: 0.5428: ep7,  best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optAdam.batch8.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_2.hdf5\n",
        "Test Acc: 0.7386, Test Loss: 0.5313: ep11, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons32.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_3.hdf5\n",
        "Test Acc: 0.7514, Test Loss: 0.5138: ep10, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last.hdf5\n",
        "-\n",
        "Test Acc: 0.8229, Test Loss: 0.4319: ep12, best_weights.BanglaSentiment_Bin_NafisAlvi.Dense.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_split0.5.hdf5\n",
        "\n",
        "Test Acc: 0.8029, Test Loss: 0.4255: ep12, best_weights.BnSent_Bin_Nafis.Dense.nFolds.2.Fold.1.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1_2.hdf5\n",
        "Test Acc: 0.6703, Test Loss: 0.6366: ep10, best_weights.BnSent_Bin_Nafis.Dense.nFolds.2.Fold.2.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1_2.hdf5\n",
        "\n",
        "Test Acc: 0.7614, Test Loss: 0.5033: ep9, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold1.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Test Acc: 0.8514, Test Loss: 0.3924: ep11, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold2.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Test Acc: 0.9071, Test Loss: 0.2626: ep13, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold3.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Test Acc: 0.8471, Test Loss: 0.327: ep11, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold4.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Test Acc: 0.8686, Test Loss: 0.3193: ep10, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold5.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Test Acc: 0.8671, Test Loss: 0.3429: ep10, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold6.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Test Acc: 0.8214, Test Loss: 0.4216: ep10, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold7.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Test Acc: 0.6214, Test Loss: 0.676: ep7, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold8.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Test Acc: 0.7871, Test Loss: 0.5899: ep13, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold9.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Test Acc: 0.5086, Test Loss: 0.6971: ep6, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold10.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run1.hdf5\n",
        "Avg Acc: 0.78412, Avg Loss: 0.45321.\n",
        "-\n",
        "Test Acc: 0.7371, Test Loss: 0.576: ep10, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold1.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "Test Acc: 0.8486, Test Loss: 0.3868: ep10, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold2.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "Test Acc: 0.8986, Test Loss: 0.2828: ep12, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold3.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "Test Acc: 0.8529, Test Loss: 0.3138: ep12, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold4.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "Test Acc: 0.87, Test Loss: 0.3458: ep11, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold5.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "Test Acc: 0.8457, Test Loss: 0.3582: ep10, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold6.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "Test Acc: 0.8143, Test Loss: 0.4031: ep9, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold7.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "Test Acc: 0.6386, Test Loss: 0.6811: ep8, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold8.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "Test Acc: 0.7171, Test Loss: 0.5966: ep8, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold9.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "Test Acc: 0.5814, Test Loss: 0.6883: ep8, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold10.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run2.hdf5\n",
        "\n",
        "-\n",
        "Test Acc: 0.7214, Test Loss: 0.5551: ep9, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold1.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run3.hdf5\n",
        "Test Acc: 0.8571, Test Loss: 0.3614: ep10, best_weights.BnSent_Bin_Nafis.Dense.nFolds10.Fold2.act.default.nLayers5.nNeurons48.optAdam.batch64.dropout0.5.loss.categorical_crossentropy.lr.0.0001_withGRU_L1_last_run3.hdf5\n",
        "\n",
        "'''\n",
        "\n",
        "result2 = model_loaded.evaluate(final_test_dataset, final_test_label)\n",
        "#print(\"nLayers: {}, nNeurons: {}, DROPOUT: {}, Test Acc: {}, Test Loss: {}\".format(NUM_LAYERS, NUM_NEURONS, DROPOUT, round(result2[1], 4), round(result2[0], 4)))\n",
        "print(\"Test Acc: {}, Test Loss: {}: ep{}, {}\\n\".format(round(result2[1],4), round(result2[0],4), epochs_completed, \"best_\"+checkpointer_name))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQi-mJcuGj6T"
      },
      "source": [
        "with open(work_dir+'Records.csv', \"a\") as fp:\n",
        "    wr = csv.writer(fp, dialect='excel')\n",
        "    try:\n",
        "        wr.writerow([checkpointer_name[8:-5], round(result2[1], 4), round(result2[0], 4), elapsed_time])\n",
        "    except:\n",
        "        wr.writerow([checkpointer_name[8:-5], round(result2[1], 4), round(result2[0], 4)])\n",
        "print(\"Saved results.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_3kbjV5Gj9Q"
      },
      "source": [
        "'''\n",
        "#Confution Matrix and Classification Report\n",
        "Y_pred = model_loaded.predict_generator(np_test_dataset, len(np_test_dataset))\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(np_test_labelset, y_pred))\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhYLA4HiGkCj"
      },
      "source": [
        "'''\n",
        "print('Classification Report:')\n",
        "#target_names = ['Mono', 'Di'] # not ['Di', 'Mono']\n",
        "print(classification_report(np_test_labelset, y_pred)) #, target_names=target_names))\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYSz-jVVGkHx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOyDmmGpGkKm"
      },
      "source": [
        "log_data = pd.read_csv(work_dir+log_name, sep=',', engine='python') \n",
        "print(\"log_data.shape =\", log_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB-euUzeGkAI"
      },
      "source": [
        "log_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru1c7H41Ge8-"
      },
      "source": [
        "'''\n",
        "# plot the training artifacts\n",
        "title = \"Val loss for \"+dataset+\" \"+impl_type+\" (\"+str(NUM_LAYERS)+\" layers, \"+str(NUM_NEURONS)+\" neurons)\"\n",
        "\n",
        "plt.plot(log_data['loss'])\n",
        "plt.plot(log_data['val_loss'])\n",
        "plt.title(title)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss','val_loss'], loc = 'best')\n",
        "\n",
        "img_path = work_dir+'Images/vLoss_'+checkpointer_name[8:-5]+'.png'\n",
        "plt.savefig(img_path, dpi=600)\n",
        "plt.show()\n",
        "print('img_path =', img_path)\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFGhgHm1G54b"
      },
      "source": [
        "'''\n",
        "title = \"Val acc for \"+dataset+\" \"+impl_type+\" (\"+str(NUM_LAYERS)+\" layers, \"+str(NUM_NEURONS)+\" neurons)\"\n",
        "\n",
        "plt.plot(log_data['accuracy'])\n",
        "plt.plot(log_data['val_accuracy'])\n",
        "plt.title(title)\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_accuracy','val_accuracy'], loc = 'best')\n",
        "\n",
        "img_path = work_dir+'Images/vAcc_'+checkpointer_name[8:-5]+'.png'\n",
        "plt.savefig(img_path, dpi=600)\n",
        "plt.show()\n",
        "print('img_path =', img_path)\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHdJbezWG57y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDBtva1PG6B6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcNO1RGfG6Es"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOXvHgOGG6LE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9A__PGuG6N4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-8QNLKXG6Ro"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eboHrOHGG6IH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcVa8bsyG5_X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}